# ğŸ¬ Netflix Prize Data Warehouse & Recommendation System

> A complete end-to-end data engineering and machine learning project for building a Netflix-scale recommendation system. From raw rating data to intelligent predictions.

---

## ğŸ“‹ Table of Contents

- [ğŸ¬ Netflix Prize Data Warehouse \& Recommendation System](#-netflix-prize-data-warehouse--recommendation-system)
  - [ğŸ“‹ Table of Contents](#-table-of-contents)
  - [ğŸ¯ Project Overview](#-project-overview)
    - [Key Metrics](#key-metrics)
  - [ğŸ“ Repository Structure](#-repository-structure)
  - [ğŸš€ Quick Start](#-quick-start)
    - [Prerequisites](#prerequisites)
    - [Setup (5 minutes)](#setup-5-minutes)
    - [Next Steps](#next-steps)
  - [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
  - [ğŸ”„ Data Flow](#-data-flow)
  - [ğŸ“¦ Submodules](#-submodules)
    - [Data Ingestion Pipeline](#data-ingestion-pipeline)
    - [Recommendation Modelling](#recommendation-modelling)
  - [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
    - [Data Engineering](#data-engineering)
    - [Machine Learning](#machine-learning)
    - [Development \& Deployment](#development--deployment)
  - [ğŸ“¥ Installation](#-installation)
    - [Option 1: Quick Setup with Docker (Recommended)](#option-1-quick-setup-with-docker-recommended)
    - [Option 2: Manual PostgreSQL Setup](#option-2-manual-postgresql-setup)
    - [Python Environment](#python-environment)
  - [ğŸ’» Usage](#-usage)
    - [Running the ETL Pipeline](#running-the-etl-pipeline)
    - [Running Recommendation Models](#running-recommendation-models)
    - [Database Queries](#database-queries)
  - [ğŸ—„ï¸ Database Schema](#ï¸-database-schema)
  - [âœ¨ Key Features](#-key-features)
    - [Data Ingestion](#data-ingestion)
    - [Recommendation System](#recommendation-system)
  - [ğŸ”€ Project Workflow](#-project-workflow)

---

## ğŸ¯ Project Overview

This repository contains a production-grade, end-to-end pipeline for analyzing and modeling the **Netflix Prize dataset** - a collection of **100M+ movie ratings** from **480K+ customers** across **17K+ movie titles** spanning from October 1998 to December 2005.

The project is divided into two complementary submodules working in tandem:

1. **Data Ingestion**: Extract, transform, and load Netflix Prize data into a dimensional data warehouse using Apache Spark
2. **Recommendation Modelling**: Build collaborative filtering recommendation models using the processed data

### Key Metrics

| Metric                   | Value                                    |
| ------------------------ | ---------------------------------------- |
| **Total Ratings**        | 100M+ records                            |
| **Unique Customers**     | 480K+                                    |
| **Unique Movies**        | 17K+ titles                              |
| **Date Range**           | Oct 1998 - Dec 2005                      |
| **Processing Framework** | Apache Spark 3.5+                        |
| **Database**             | PostgreSQL 12+                           |
| **Models**               | SVD, ALS (Spark MLlib)                   |
| **Graph Analytics**      | Community detection, centrality analysis |

---

## ğŸ“ Repository Structure

```
netflix/
â”œâ”€â”€ README.md                           # This file - Project overview
â”œâ”€â”€ data-ingestion/                     # ETL Pipeline (Submodule 1)
â”‚   â”œâ”€â”€ README.md                       # Detailed ETL documentation
â”‚   â”œâ”€â”€ etl_pipeline_spark.py          # Main Spark ETL pipeline
â”‚   â”œâ”€â”€ schema.sql                      # PostgreSQL database schema
â”‚   â”œâ”€â”€ docker-compose.yml              # Local PostgreSQL setup
â”‚   â”œâ”€â”€ Dockerfile                      # Spark container image
â”‚   â”œâ”€â”€ pyproject.toml                  # Project metadata
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â”œâ”€â”€ data/                           # Raw dataset files
â”‚   â”œâ”€â”€ checkpoints/                    # Resumable pipeline checkpoints
â”‚   â”œâ”€â”€ logs/                           # Pipeline execution logs
â”‚   â””â”€â”€ temp_csv/ & temp_parquet/       # Intermediate data storage
â”‚
â””â”€â”€ modelling/                          # Recommendation Models (Submodule 2)
    â”œâ”€â”€ README.md                       # Detailed modelling documentation
    â”œâ”€â”€ model.ipynb                     # Jupyter notebook with full analysis
    â”œâ”€â”€ requirements.txt                # Python dependencies
    â”œâ”€â”€ about_dataset.txt               # Dataset information
    â”œâ”€â”€ assignment.txt                  # Project assignment details
    â””â”€â”€ schemas.txt                     # Database schema reference
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- PostgreSQL 12+ (or use Docker)
- Apache Spark 3.5+
- Git

### Setup (5 minutes)

```bash
# Clone the repository
git clone <repository-url>
cd netflix

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or: .\venv\Scripts\Activate.ps1  # Windows

# Install data-ingestion dependencies
cd data-ingestion
pip install -r requirements.txt

# Set up PostgreSQL (Option A: Docker)
docker-compose up -d

# Create database schema
psql -h localhost -U postgres -f schema.sql
# Enter password when prompted

# Configure environment
cp .env.example .env
# Edit .env with your database credentials

# Run ETL pipeline
python etl_pipeline_spark.py
```

### Next Steps

After the ETL completes:

```bash
# Navigate to modelling submodule
cd ../modelling
pip install -r requirements.txt

# Open and run the Jupyter notebook
jupyter notebook model.ipynb
```

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NETFLIX PRIZE DATASET                         â”‚
â”‚  (Raw Files: movie_titles.csv, probe.txt, qualifying.txt, etc)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   DATA INGESTION PIPELINE          â”‚
        â”‚   (Apache Spark + PySpark)         â”‚
        â”‚                                    â”‚
        â”‚  â€¢ Extract raw rating data         â”‚
        â”‚  â€¢ Transform to Star Schema        â”‚
        â”‚  â€¢ Load into PostgreSQL            â”‚
        â”‚  â€¢ Resumable checkpoints           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   DIMENSIONAL DATA WAREHOUSE       â”‚
        â”‚   (PostgreSQL)                     â”‚
        â”‚                                    â”‚
        â”‚  â€¢ fact_ratings                    â”‚
        â”‚  â€¢ dim_customer                    â”‚
        â”‚  â€¢ dim_movie                       â”‚
        â”‚  â€¢ dim_date                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   RECOMMENDATION MODELLING         â”‚
        â”‚   (Scikit-Learn + Spark MLlib)     â”‚
        â”‚                                    â”‚
        â”‚  â€¢ Collaborative Filtering         â”‚
        â”‚  â€¢ Matrix Factorization (SVD)      â”‚
        â”‚  â€¢ ALS (Alternating Least Squares) â”‚
        â”‚  â€¢ Graph Analytics                 â”‚
        â”‚  â€¢ Performance Metrics             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Flow

```
Raw Ratings Data
    â†“
[Data Validation] â†’ Remove nulls/duplicates
    â†“
[Spark Transformation] â†’ Normalize, aggregate, compute features
    â†“
[Star Schema Mapping] â†’ Create fact/dimension tables
    â†“
[PostgreSQL Load] â†’ Insert into data warehouse
    â†“
[Checkpoint System] â†’ Save progress for resumability
    â†“
[Data Ready for Modeling]
    â†“
[Feature Engineering] â†’ User/movie bias, normalized ratings
    â†“
[Model Training] â†’ SVD, ALS hyperparameter tuning
    â†“
[Model Evaluation] â†’ MAE, RMSE, MAPE metrics
    â†“
[Recommendations Generated]
```

---

## ğŸ“¦ Submodules

### Data Ingestion Pipeline

**Location**: [data-ingestion/](data-ingestion/)

A production-grade ETL pipeline that processes 100M+ Netflix ratings into a normalized data warehouse.

**Key Features**:

- âœ… Resumable processing with automatic checkpoints
- âœ… Duplicate-safe data insertion
- âœ… Star Schema dimensional modeling
- âœ… Apache Spark for distributed processing
- âœ… Real-time progress tracking
- âœ… Docker support for quick setup

**Main Components**:

- `etl_pipeline_spark.py`: Core ETL orchestration
- `schema.sql`: PostgreSQL dimensional schema
- `docker-compose.yml`: Local development environment

For detailed documentation, see [data-ingestion/README.md](data-ingestion/README.md)

### Recommendation Modelling

**Location**: [modelling/](modelling/)

Machine learning models and graph analytics for Netflix-scale recommendation systems.

**Key Features**:

- âœ… Collaborative filtering implementation
- âœ… Matrix factorization (SVD)
- âœ… Spark MLlib ALS for scalability
- âœ… Hyperparameter tuning with GridSearch
- âœ… Comprehensive evaluation metrics
- âœ… Graph analytics & community detection

**Main Components**:

- `model.ipynb`: Complete Jupyter notebook with all analyses
- Feature engineering and data quality checks
- Model performance comparison
- Network visualization and community detection

For detailed documentation, see [modelling/README.md](modelling/README.md)

---

## ğŸ› ï¸ Tech Stack

### Data Engineering

- **Apache Spark 3.5+** - Distributed data processing
- **PySpark** - Spark Python API
- **PostgreSQL 12+** - Data warehouse
- **SQLAlchemy** - ORM and SQL toolkit
- **Pandas** - Data manipulation

### Machine Learning

- **Scikit-Learn** - Traditional ML algorithms (SVD)
- **Apache Spark MLlib** - Distributed ML (ALS)
- **NetworkX** - Graph analytics
- **Jupyter** - Interactive notebooks

### Development & Deployment

- **Docker** - Containerization
- **Python-dotenv** - Environment configuration
- **Git** - Version control

---

## ğŸ“¥ Installation

### Option 1: Quick Setup with Docker (Recommended)

```bash
# Navigate to data-ingestion
cd data-ingestion

# Start PostgreSQL
docker-compose up -d

# The database is ready on localhost:5432
```

### Option 2: Manual PostgreSQL Setup

```bash
# Install PostgreSQL (macOS with Homebrew)
brew install postgresql

# Start services
brew services start postgresql

# Create database
createdb netflix_warehouse

# Load schema
psql netflix_warehouse -f schema.sql
```

### Python Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install all dependencies
pip install -r data-ingestion/requirements.txt
pip install -r modelling/requirements.txt
```

---

## ğŸ’» Usage

### Running the ETL Pipeline

```bash
cd data-ingestion

# First run
python etl_pipeline_spark.py

# Resume after interruption (automatic)
python etl_pipeline_spark.py

# Start fresh (delete checkpoint)
rm etl_checkpoint.json
python etl_pipeline_spark.py
```

### Running Recommendation Models

```bash
cd modelling

# Start Jupyter server
jupyter notebook

# Open model.ipynb and run cells in sequence
```

### Database Queries

```bash
# Connect to database
psql -h localhost -U postgres -d netflix_warehouse

# Example queries
SELECT COUNT(*) FROM fact_ratings;
SELECT COUNT(DISTINCT customer_id) FROM dim_customer;
SELECT COUNT(DISTINCT movie_id) FROM dim_movie;
```

---

## ğŸ—„ï¸ Database Schema

The data warehouse follows a **Star Schema** design optimized for analytical queries:

```sql
-- Fact Table
fact_ratings (
  rating_key INT PRIMARY KEY,
  customer_key INT (FK),
  movie_key INT (FK),
  date_key INT (FK),
  rating INT,
  ...
)

-- Dimensions
dim_customer (customer_key, customer_id, ...)
dim_movie (movie_key, movie_id, title, year, ...)
dim_date (date_key, date, year, month, day, ...)
```

See [data-ingestion/schema.sql](data-ingestion/schema.sql) for complete schema definition.

---

## âœ¨ Key Features

### Data Ingestion

- **ğŸ“Š Distributed Processing**: Apache Spark handles 100M+ records efficiently
- **ğŸ”„ Resumable**: Automatic checkpoint system prevents data loss
- **ğŸ›¡ï¸ Data Quality**: Validation, duplicate detection, null handling
- **ğŸ“ˆ Scalable**: Star Schema design supports analytical queries
- **ğŸ³ Container Ready**: Docker setup for reproducibility

### Recommendation System

- **ğŸ¤– Multiple Algorithms**: SVD (local), ALS (distributed)
- **ğŸ”§ Hyperparameter Tuning**: GridSearch for optimal models
- **ğŸ“‰ Comprehensive Metrics**: MAE, RMSE, MAPE, MSE evaluation
- **ğŸ•¸ï¸ Graph Analytics**: Network analysis, community detection
- **ğŸ“Š Visual Analysis**: Performance charts and network visualizations

---

## ğŸ”€ Project Workflow

```
Phase 1: Data Ingestion
â”œâ”€â”€ Extract raw Netflix Prize data files
â”œâ”€â”€ Validate and clean data
â”œâ”€â”€ Transform to dimensional model
â”œâ”€â”€ Load into PostgreSQL
â””â”€â”€ Create checkpoint for resumability

Phase 2: Data Exploration
â”œâ”€â”€ Query data warehouse
â”œâ”€â”€ Compute user/movie statistics
â”œâ”€â”€ Analyze rating distributions
â””â”€â”€ Validate data quality

Phase 3: Feature Engineering
â”œâ”€â”€ Calculate user bias (avg rating)
â”œâ”€â”€ Calculate movie bias (avg rating)
â”œâ”€â”€ Normalize features for models
â””â”€â”€ Create train/test splits

Phase 4: Model Training
â”œâ”€â”€ Implement SVD (Scikit-Learn)
â”œâ”€â”€ Implement ALS (Spark MLlib)
â”œâ”€â”€ Hyperparameter tuning
â”œâ”€â”€ Cross-validation
â””â”€â”€ Save trained models

Phase 5: Evaluation & Analysis
â”œâ”€â”€ Calculate performance metrics
â”œâ”€â”€ Compare model performance
â”œâ”€â”€ Generate recommendations
â”œâ”€â”€ Graph analytics
â””â”€â”€ Visualize results
```
